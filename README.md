# Neural-Network-From-Scratch

The goal of this project is to build a neural network from scratch without using high-level deep learning libraries like TensorFlow or PyTorch. The implementation focuses on:

Forward Propagation: Computing the output of the network.

Backpropagation: Calculating gradients for weight updates.

Optimization: Using Stochastic Gradient Descent (SGD) to minimize the loss function.

Activation Functions: ReLU for the hidden layer and Softmax for the output layer.

Regularization: Optional L2 regularization to prevent overfitting.

This project is a great way to understand the foundational concepts of neural networks and deep learning.

